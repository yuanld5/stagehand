/**
 * This class is responsible for evaluating the result of an agentic task.
 * The first version includes a VLM evaluator specifically prompted to evaluate the state of a task
 * usually represented as a screenshot.
 * The evaluator will reply with YES or NO given the state of the provided task.
 */

import {
  AvailableModel,
  ClientOptions,
  Stagehand,
} from "@browserbasehq/stagehand";
import dotenv from "dotenv";
import {
  EvaluateOptions,
  BatchAskOptions,
  EvaluationResult,
} from "@/types/evaluator";
import { LLMParsedResponse } from "@/lib/inference";
import { LLMResponse } from "@/lib/llm/LLMClient";
import { LogLine } from "@/types/log";
import { z } from "zod";

dotenv.config();

const EvaluationSchema = z.object({
  evaluation: z.enum(["YES", "NO"]),
  reasoning: z.string(),
});

const BatchEvaluationSchema = z.array(EvaluationSchema);

export class Evaluator {
  private stagehand: Stagehand;
  private modelName: AvailableModel;
  private modelClientOptions: ClientOptions | { apiKey: string };
  private silentLogger: (message: LogLine) => void;

  constructor(
    stagehand: Stagehand,
    modelName?: AvailableModel,
    modelClientOptions?: ClientOptions,
  ) {
    this.stagehand = stagehand;
    this.modelName = modelName || "google/gemini-2.5-flash";
    this.modelClientOptions = modelClientOptions || {
      apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY || "",
    };
  }

  async ask(options: EvaluateOptions): Promise<EvaluationResult> {
    const {
      question,
      answer,
      screenshot = true,
      systemPrompt,
      screenshotDelayMs = 250,
    } = options;
    if (!question) {
      throw new Error("Question cannot be an empty string");
    }
    if (!answer && !screenshot) {
      throw new Error("Either answer (text) or screenshot must be provided");
    }

    // Handle multiple screenshots case
    if (Array.isArray(screenshot)) {
      return this._evaluateWithMultipleScreenshots({
        question,
        screenshots: screenshot,
        systemPrompt,
      });
    }

    // Single screenshot case (existing logic)
    const defaultSystemPrompt = `You are an expert evaluator that confidently returns YES or NO given a question and the state of a task (in the form of a screenshot, or an answer). Provide a detailed reasoning for your answer.
          Be critical about the question and the answer, the slightest detail might be the difference between yes and no. for text, be lenient and allow for slight variations in wording. we will be comparing the agents trajectory to see if it contains the information we were looking for in the answer.
          Today's date is ${new Date().toLocaleDateString()}`;

    await new Promise((resolve) => setTimeout(resolve, screenshotDelayMs));
    let imageBuffer: Buffer;
    if (screenshot) {
      imageBuffer = await this.stagehand.page.screenshot();
    }
    const llmClient = this.stagehand.llmProvider.getClient(
      this.modelName,
      this.modelClientOptions,
    );

    const response = await llmClient.createChatCompletion<
      LLMParsedResponse<LLMResponse>
    >({
      logger: this.silentLogger,
      options: {
        messages: [
          { role: "system", content: systemPrompt || defaultSystemPrompt },
          {
            role: "user",
            content: [
              { type: "text", text: question },
              ...(screenshot
                ? [
                    {
                      type: "image_url",
                      image_url: {
                        url: `data:image/jpeg;base64,${imageBuffer.toString("base64")}`,
                      },
                    },
                  ]
                : []),
              ...(answer
                ? [
                    {
                      type: "text",
                      text: `the answer is ${answer}`,
                    },
                  ]
                : []),
            ],
          },
        ],
        response_model: {
          name: "EvaluationResult",
          schema: EvaluationSchema,
        },
      },
    });

    try {
      const result = response.data as unknown as z.infer<
        typeof EvaluationSchema
      >;
      return { evaluation: result.evaluation, reasoning: result.reasoning };
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      return {
        evaluation: "INVALID" as const,
        reasoning: `Failed to get structured response: ${errorMessage}`,
      };
    }
  }

  /**
   * Evaluates multiple questions with optional answers and/or screenshot.
   * Similar to ask() but processes multiple questions in a single call.
   * Returns an array of evaluation results.
   *
   * @param options - The options for batch evaluation
   * @returns A promise that resolves to an array of EvaluationResults
   */
  async batchAsk(options: BatchAskOptions): Promise<EvaluationResult[]> {
    const {
      questions,
      screenshot = true,
      systemPrompt = `You are an expert evaluator that confidently returns YES or NO for each question given the state of a task (in the form of a screenshot, or an answer). Provide a detailed reasoning for your answer.
           Be critical about the question and the answer, the slightest detail might be the difference between yes and no. for text, be lenient and allow for slight variations in wording. we will be comparing the agents trajectory to see if it contains the information we were looking for in the answer.
          Today's date is ${new Date().toLocaleDateString()}`,
      screenshotDelayMs = 1000,
    } = options;

    // Validate inputs
    if (!questions || questions.length === 0) {
      throw new Error("Questions array cannot be empty");
    }

    for (const item of questions) {
      if (!item.question) {
        throw new Error("Question cannot be an empty string");
      }
      if (!item.answer && !screenshot) {
        throw new Error(
          "Either answer (text) or screenshot must be provided for each question",
        );
      }
    }

    // Wait for the specified delay before taking screenshot
    await new Promise((resolve) => setTimeout(resolve, screenshotDelayMs));

    let imageBuffer: Buffer;
    if (screenshot) {
      imageBuffer = await this.stagehand.page.screenshot();
    }

    // Get the LLM client with our preferred model
    const llmClient = this.stagehand.llmProvider.getClient(
      this.modelName,
      this.modelClientOptions,
    );

    // Format all questions with their optional answers
    const formattedQuestions = questions
      .map((item, i) => {
        let text = `${i + 1}. ${item.question}`;
        if (item.answer) {
          text += `\n   Answer: ${item.answer}`;
        }
        return text;
      })
      .join("\n\n");

    // Use the model-specific LLM client to evaluate
    const response = await llmClient.createChatCompletion<
      LLMParsedResponse<LLMResponse>
    >({
      logger: this.silentLogger,
      options: {
        messages: [
          {
            role: "system",
            content: `${systemPrompt}\n\nYou will be given multiple questions${screenshot ? " with a screenshot" : ""}. ${questions.some((q) => q.answer) ? "Some questions include answers to evaluate." : ""} Answer each question by returning an object in the specified JSON format. Return a single JSON array containing one object for each question in the order they were asked.`,
          },
          {
            role: "user",
            content: [
              { type: "text", text: formattedQuestions },
              ...(screenshot
                ? [
                    {
                      type: "image_url",
                      image_url: {
                        url: `data:image/jpeg;base64,${imageBuffer.toString("base64")}`,
                      },
                    },
                  ]
                : []),
            ],
          },
        ],
        response_model: {
          name: "BatchEvaluationResult",
          schema: BatchEvaluationSchema,
        },
      },
    });

    try {
      const results = response.data as unknown as z.infer<
        typeof BatchEvaluationSchema
      >;
      return results.map((r) => ({
        evaluation: r.evaluation,
        reasoning: r.reasoning,
      }));
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      return questions.map(() => ({
        evaluation: "INVALID" as const,
        reasoning: `Failed to get structured response: ${errorMessage}`,
      }));
    }
  }

  /**
   * Private method to evaluate with multiple screenshots
   */
  private async _evaluateWithMultipleScreenshots(options: {
    question: string;
    screenshots: Buffer[];
    systemPrompt?: string;
  }): Promise<EvaluationResult> {
    const {
      question,
      screenshots,
      systemPrompt = `You are an expert evaluator that confidently returns YES or NO given a question and multiple screenshots showing the progression of a task.
        Analyze ALL screenshots to understand the complete journey. Look for evidence of task completion across all screenshots, not just the last one.
        Success criteria may appear at different points in the sequence (confirmation messages, intermediate states, etc).
        Be critical about the question but consider the ENTIRE sequence when making your determination.
        Today's date is ${new Date().toLocaleDateString()}`,
    } = options;

    if (!question) {
      throw new Error("Question cannot be an empty string");
    }

    if (!screenshots || screenshots.length === 0) {
      throw new Error("At least one screenshot must be provided");
    }

    const llmClient = this.stagehand.llmProvider.getClient(
      this.modelName,
      this.modelClientOptions,
    );

    const imageContents = screenshots.map((screenshot) => ({
      type: "image_url" as const,
      image_url: {
        url: `data:image/jpeg;base64,${screenshot.toString("base64")}`,
      },
    }));

    const response = await llmClient.createChatCompletion<
      LLMParsedResponse<LLMResponse>
    >({
      logger: this.silentLogger,
      options: {
        messages: [
          { role: "system", content: systemPrompt },
          {
            role: "user",
            content: [
              {
                type: "text",
                text: `${question}\n\nI'm providing ${screenshots.length} screenshots showing the progression of the task. Please analyze all of them to determine if the task was completed successfully.`,
              },
              ...imageContents,
            ],
          },
        ],
        response_model: {
          name: "EvaluationResult",
          schema: EvaluationSchema,
        },
      },
    });

    try {
      const result = response.data as unknown as z.infer<
        typeof EvaluationSchema
      >;
      return { evaluation: result.evaluation, reasoning: result.reasoning };
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      return {
        evaluation: "INVALID" as const,
        reasoning: `Failed to get structured response: ${errorMessage}`,
      };
    }
  }
}
